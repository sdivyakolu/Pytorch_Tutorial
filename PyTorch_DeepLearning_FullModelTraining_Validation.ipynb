{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOapaae9MsjigCVQE/H9ghh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdivyakolu/Pytorch_Tutorial/blob/main/PyTorch_DeepLearning_FullModelTraining_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AGeyvm3kwi9u"
      },
      "outputs": [],
      "source": [
        "#Model Development Traning ( SOLUTIONING THE PROBLEM : Most important step)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import dataloader\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "from torch import optim\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define neural network , __init__ and forward functions.\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.conv1=nn.Conv2d(3,6, 5)\n",
        "    self.pool=nn.MaxPool2d(2,2)\n",
        "    self.conv2=nn.Conv2d(6,16,5)\n",
        "    self.fc1=nn.Linear(16*5*5,120)\n",
        "    self.fc2=nn.Linear(120,84)\n",
        "    self.fc3=nn.Linear(84,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.pool(F.relu(self.conv1(x)))\n",
        "    x=self.pool(F.relu(self.conv2(x)))\n",
        "    x=x.view(-1,16*5*5)\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=F.relu(self.fc2(x))\n",
        "    x=self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "bJ5ByGGxwtYU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create model class and initiate it\n",
        "net=Net()\n",
        "print(net)\n",
        "\n",
        "#Define loss function and Optimizer\n",
        "#lr=0.4\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96Yr_RXvwtVJ",
        "outputId": "a0ae4d32-12aa-4122-ea7a-1588289e6312"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Load and trasnform the data\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize(std=(0.5,0.5,0.5),\n",
        "                                                                    mean=(0.5,0.5,0.5))])\n",
        "\n",
        "train_trasforms=transforms.Compose([transforms.RandomCrop(32,padding=4),\n",
        "                                    transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=(0.4914,0.4822,0.4465),\n",
        "                                                         std=(0.2023,0.1994,0.2010))])\n",
        "\n",
        "batch_size=32\n",
        "train_size=400\n",
        "val_size=100\n",
        "\n",
        "\n",
        "\n",
        "train_data=CIFAR10.CIFAR10(root='./data',train=True,download=True,transform=train_trasforms)\n",
        "test_set=CIFAR10.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "#train_set, validation_set=random_split(train_data,[train_size,val_size])\n",
        "\n",
        "# Create subsets\n",
        "train_sub_data = Subset(train_data, range(4000))   # First 400 samples\n",
        "test_validation_sub_set  = Subset(test_set, range(1000))    # First 100 samples\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(train_sub_data,batch_size=batch_size,shuffle=True,num_workers=2)\n",
        "\n",
        "validation_loader=torch.utils.data.DataLoader(test_validation_sub_set,batch_size=batch_size,shuffle=False,num_workers=2)\n",
        "\n",
        "#Example:Check the length of splits\n",
        "print(\"Traning Set Size:\",{len(train_sub_data)})\n",
        "print(\"Validation Set Size:\",{len(test_validation_sub_set)})\n",
        "\n",
        "\n",
        "testloader=torch.utils.data.DataLoader(test_validation_sub_set,batch_size=batch_size,shuffle=False,num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXwI3b_QwtQz",
        "outputId": "9b5d38b3-7793-43f3-8de8-dbc275d9e1fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traning Set Size: {4000}\n",
            "Validation Set Size: {1000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the network/Model\n",
        "epochs=10\n",
        "for epoch in range(epochs):\n",
        "  net.train() #set the model to training mode\n",
        "  running_loss=0.0\n",
        "  for i,data in enumerate(trainloader,0):\n",
        "\n",
        "    #get the inputs, data is a triple of [inputs,labels]\n",
        "    inputs,labels=data\n",
        "\n",
        "    #zero the parameter gradiant\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #farward + backward = optimize\n",
        "    outputs=net(inputs)\n",
        "    loss=criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #Print Statstics\n",
        "    running_loss+=loss.item()\n",
        "    net.eval()  #set the model to evaluation mode for validation\n",
        "    validation_loss=0.0\n",
        "    correct=0\n",
        "    total=0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "          images,labels=data\n",
        "          outputs=net(images)\n",
        "          loss=criterion(outputs,labels)\n",
        "          validation_loss +=loss.item()\n",
        "          _,predicted=torch.max(outputs.data,1)\n",
        "          total+=labels.size(0)\n",
        "          correct += ( predicted==labels).sum().item()\n",
        "  print(f'epoch: {epoch+1}, training_loss: {running_loss/len(trainloader)}, validation_loss: {validation_loss/len(validation_loader)}, accuracy: {100*correct/total}%')\n",
        "print('Fininsh Traning')"
      ],
      "metadata": {
        "id": "GAeYRZcIwtNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f05d4a3-7976-47ca-da87-630d2851520a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, training_loss: 2.277782241821289, validation_loss: 2.274771213531494, accuracy: 10.4%\n",
            "epoch: 2, training_loss: 2.2050089025497437, validation_loss: 2.2154005616903305, accuracy: 24.6%\n",
            "epoch: 3, training_loss: 2.0990346307754515, validation_loss: 2.158205658197403, accuracy: 25.1%\n",
            "epoch: 4, training_loss: 2.0214395513534544, validation_loss: 2.0931660532951355, accuracy: 27.6%\n",
            "epoch: 5, training_loss: 1.9820764312744141, validation_loss: 2.084401521831751, accuracy: 27.4%\n",
            "epoch: 6, training_loss: 1.9542114973068236, validation_loss: 2.0534238442778587, accuracy: 27.6%\n",
            "epoch: 7, training_loss: 1.919598711013794, validation_loss: 2.0383739806711674, accuracy: 29.7%\n",
            "epoch: 8, training_loss: 1.8982076368331908, validation_loss: 2.0036757215857506, accuracy: 30.5%\n",
            "epoch: 9, training_loss: 1.8589166975021363, validation_loss: 2.001039583235979, accuracy: 32.0%\n",
            "epoch: 10, training_loss: 1.8413868808746339, validation_loss: 1.9769608713686466, accuracy: 32.1%\n",
            "Fininsh Traning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I9XJ9qKcwtLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRgOa_M3wtHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3ZY2MgvwtDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}